<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>EconDL | NEWS-COPY</title>
    <meta name="description" content="Silcock, Emily, Luca D'Amico-Wong, Jinglin Yang, and Melissa Dell. “Noise-Robust De-Duplication at Scale", *International Conference on Learning Representations*, vol. 332 (2023). [Paper](https://scholar.harvard.edu/files/dell/files/iclr_samesource_revision_non_anonymous_1.pdf)
">

    <meta property="og:title" content="NEWS-COPY">
    <meta property="og:type" content="website">
    <meta property="og:url" content="/projects/394dups">
    <meta property="og:description" content="Silcock, Emily, Luca D'Amico-Wong, Jinglin Yang, and Melissa Dell. “Noise-Robust De-Duplication at Scale", *International Conference on Learning Representations*, vol. 332 (2023). [Paper](https://scholar.harvard.edu/files/dell/files/iclr_samesource_revision_non_anonymous_1.pdf)
">
    <meta property="og:site_name" content="">

    <meta name="twitter:card" content="summary">
    <meta name="twitter:url" content="/projects/394dups">
    <meta name="twitter:title" content="NEWS-COPY">
    <meta name="twitter:description" content="Silcock, Emily, Luca D'Amico-Wong, Jinglin Yang, and Melissa Dell. “Noise-Robust De-Duplication at Scale", *International Conference on Learning Representations*, vol. 332 (2023). [Paper](https://scholar.harvard.edu/files/dell/files/iclr_samesource_revision_non_anonymous_1.pdf)
">

    <!-- TODO: Adding page thumbnail for social meida -->

    <!-- Canonical URL -->
    <link rel="canonical" href="/projects/394dups">

    <!-- Stylesheet -->
    <link rel="stylesheet" href="/assets/css/bulma.css">
    <link rel="stylesheet" href="/assets/css/syntax/manni.css">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

    <script defer src="https://use.fontawesome.com/releases/v5.3.1/js/all.js"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
          inlineMath: [ ['$','$'] ],
          displayMath: [ ['$$','$$'] ],
          processEscapes: true,
        }
      });
    </script>
    
    <script type="text/javascript"
            src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    
    <!-- Add link to h2 ... h6 -->
    <!-- Ref: https://ben.balter.com/2014/03/13/pages-anchor-links/ -->
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script>
      $(function() {
        return $("h2, h3, h4, h5, h6").each(function(i, el) {
          var $el, icon, id;
          $el = $(el);
          id = $el.attr('id');
          icon = '<i class="fas fa-link"></i>';
          if (id) {
            return $el.append($("<a />").addClass("header-link").attr("href", "#" + id).html(icon));
          }
        });
      });
    </script>
    
    <script src="/assets/js/bulma-collapsible.min.js"></script>

    <!-- Google Analytics -->
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', '');
</script>


</head>

<body>

    <div class="section">            
        <div class="container">
            
                <nav class="navbar" role="navigation">
    <div class="navbar-brand">
        <a class="navbar-item site-title" href="/">
            <strong>EconDL</strong>
        </a>

        <div class="navbar-burger" data-target="navbar-main"
            onclick="document.querySelector('.navbar-menu').classList.toggle('is-active');">
            <span></span>
            <span></span>
            <span></span>
        </div>
    </div>

    <div class="navbar-menu" id="navbar-main">
        <div class="navbar-end">
            <!-- navbar items -->
            
                
                    <a href="/" class="navbar-item"></a>
                
            
                
                    <a href="/packages.html" class="navbar-item">Packages</a>
                
            
                
                    <a href="/datasets.html" class="navbar-item">Datasets</a>
                
            
            <div class="navbar-item">
                
            </div>
        </div>
    </div>
</nav>
<div class="is-divider" style="margin-top: 0.5rem; margin-bottom: 3rem;"></div>
  
            
            <article class="content" style="margin-top: 3rem; margin-bottom: 4rem">
  <header class="article-header">
      <div class="columns">
        <div class="column is-12">
          <h1 style="letter-spacing: -0.02rem">NEWS-COPY</h1>
           
<div class="article-list-footer">
    
    <!-- <span class="article-list-divider">-</span>
    <span class="field is-grouped">
        
    </span> -->
</div>
        </div>
      </div>
  </header>
  <div class="is-divider" style="margin-top: 0.5rem; margin-bottom: 1.5rem;"></div>
  <p><strong>Abstract:</strong> Identifying near duplicates within large, noisy text corpora has a myriad of applications that range from de-duplicating training datasets, reducing privacy risk, and evaluating test set leakage, to identifying reproduced news articles and literature within large corpora. Across these diverse applications, the overwhelming majority of work relies on N-grams. Limited efforts have been made to evaluate how well N-gram methods perform, in part because it is unclear how one could create an unbiased evaluation dataset for a massive corpus. This study uses the unique timeliness of historical news wires to create a 27,210 document dataset, with 122,876 positive duplicate pairs, for studying noise-robust de-duplication. The time-sensitivity of news makes comprehensive hand labelling feasible - despite the massive overall size of the corpus - as duplicates occur within a narrow date range. The study then develops and evaluates a range of de-duplication methods: hashing and N-gram overlap (which predominate in the literature), a contrastively trained bi-encoder, and a ``re-rank’’ style approach combining a bi- and cross-encoder. The neural approaches significantly outperform hashing and N-gram overlap. We show that the bi-encoder scales well, de-duplicating a 10 million article corpus on a single GPU card in a matter of hours. We also apply our pre-trained model to the RealNews and patent portions of C4 (Colossal Clean Crawled Corpus), illustrating that a neural approach can identify many near duplicates missed by hashing, in the presence of various types of noise. The public release of our NEWS-COPY de-duplication dataset, de-duplicated RealNews and patent corpuses, and the pre-trained models will facilitate further research and applications.</p>

<p><a href="https://scholar.harvard.edu/files/dell/files/iclr_samesource_revision_non_anonymous_1.pdf">Paper</a></p>

</article>

        </div>
    </div>
    

</body>

</html>