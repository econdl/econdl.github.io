---
layout: post
description: >
    Lecture 9: Prompt and Prefix Tuning 
title: Prompt and Prefix Tuning
category: intro
tags: [deep learning]
image: /assets/blogs/lecture_9.png
mathjax: true
published: true
---

## Topics

This post covers the ninth lecture in the course: "Prompt and Prefix Tuning." 

Prompt tuning, the idea of framing everything as a text prediction task using an enormous, frozen LLM (i.e. GPT3) has gained a lot of popularity in recent years. This lecture covers both discrete prompt tuning and continuous prompt tuning, also called prefix tuning. 

## Lecture Video

<a href="https://www.youtube.com/watch?v=3vb2I44zKWI&ab_channel=MelissaDell" target="_blank">
 <img src="http://img.youtube.com/vi/3vb2I44zKWI/mqdefault.jpg" alt="Watch the video" width="560" height="315" />
</a>

[Lecture notes](https://www.dropbox.com/s/umbc2xwsqaxeq0n/lecture_prompting.pdf?dl=0)


## References Cited in Lecture 8: Contrastive Learning

_Prompting and Prefix Tuning_
 
Liu, Pengfei, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. "[Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.](https://dl.acm.org/doi/pdf/10.1145/3560815)" ACM Computing Surveys 55, no. 9 (2023): 1-35. 
 
Wei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. "[Chain of thought prompting elicits reasoning in large language models.](https://arxiv.org/pdf/2201.11903.pdf?trk=public_post_comment-text)" arXiv preprint arXiv:2201.11903 (2022). 
 
Khattab, Omar, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. "[Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP.](https://arxiv.org/pdf/2212.14024.pdf)" arXiv preprint arXiv:2212.14024 (2022). 
 
Li, Xiang Lisa, and Percy Liang. "[Prefix-tuning: Optimizing continuous prompts for generation.](https://arxiv.org/pdf/2101.00190.pdf)" arXiv preprint arXiv:2101.00190 (2021). 
 
Lester, Brian, Rami Al-Rfou, and Noah Constant. "[The power of scale for parameter-efficient prompt tuning.](https://arxiv.org/pdf/2104.08691.pdf)" arXiv preprint arXiv:2104.08691 (2021).

#### Other Resources

_Image Source_: https://thumtblog.github.io/2022/04/05/robust-prefix-tuning/