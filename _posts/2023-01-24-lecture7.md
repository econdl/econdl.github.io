---
layout: post
description: >
    Lecture 7: Covers the basics of training and optimizing neural networks
title: Basics of Training and Optimizing Neural Networks
category: intro
tags: [deep learning]
image: /assets/blogs/lecture_7.jpg
mathjax: true
published: true
---

## Topics

This post covers the seventh lecture in the course: "Basics of Optimizing Neural Networks; Classification." 

This is a kitchen sink lecture about optimizing neural nets. The Goodfellow, Bengio and Courville textbook is a very helpful reference. Here are some additional references that provide very useful insights: 

## Lecture Video

<a href="https://www.youtube.com/watch?v=TgXkNt4Z6I0&ab_channel=MelissaDell" target="_blank">
 <img src="http://img.youtube.com/vi/TgXkNt4Z6I0/mqdefault.jpg" alt="Watch the video" width="560" height="315" />
</a>

[Lecture notes](https://www.dropbox.com/s/awi97039q0a5f32/lecture_train_nns.pdf?dl=0)


## References Cited in Lecture 7: Basics of Optimizing Neural Networks; classification

_Momentum_ 

- Goh, Gabriel. "[Why momentum really works](https://distill.pub/2017/momentum/?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=email&utm_source=Artificial_Intelligence_Weekly_59)." Distill 2, no. 4 (2017): e6 (excellent treatment of momentum). 

_Batch Normalization_

- Ioffe, Sergey, and Christian Szegedy. "[Batch normalization: Accelerating deep network training by reducing internal covariate shift.](http://proceedings.mlr.press/v37/ioffe15.pdf)" In International conference on machine learning, pp. 448-456. PMLR, 2015.

- Santurkar, Shibani, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. "[How does batch normalization help optimization?.](https://proceedings.neurips.cc/paper/2018/file/905056c1ac1dad141560467e0a99e1cf-Paper.pdf)" Advances in Neural Information Processing Systems (2018).

_Other Refinements_

- He, Tong, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. "[Bag of tricks for image classification with convolutional neural networks.](https://openaccess.thecvf.com/content_CVPR_2019/papers/He_Bag_of_Tricks_for_Image_Classification_with_Convolutional_Neural_Networks_CVPR_2019_paper.pdf)" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 558-567. 2019.

_Hyperparameter Tuning_

- Li, Hao, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. "[Visualizing the loss landscape of neural nets.](https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf)" Advances in Neural Information Processing Systems (2018).

- Li, Lisha, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. "[Hyperband: A novel bandit-based approach to hyperparameter optimization.](https://www.jmlr.org/papers/volume18/16-558/16-558.pdf)" The Journal of Machine Learning Research 18, no. 1 (2017): 6765-6816.

- Falkner, Stefan, Aaron Klein, and Frank Hutter. "[BOHB: Robust and efficient hyperparameter optimization at scale.](http://proceedings.mlr.press/v80/falkner18a/falkner18a.pdf)" In International Conference on Machine Learning, pp. 1437-1446. PMLR, 2018.

- Frankle, Jonathan, and Michael Carbin. "[The lottery ticket hypothesis: Finding sparse, trainable neural networks.](https://arxiv.org/pdf/1803.03635.pdf)" arXiv preprint arXiv:1803.03635 (2018).

#### Other Resources

- Use [Weights & Biases](https://wandb.ai/site)

- Weights and Biases: [Running Hyperparameter Sweeps to Pick the Best Model](https://wandb.ai/site/articles/running-hyperparameter-sweeps-to-pick-the-best-model-using-w-b#:~:text=Weights%20and%20biases%20are%20the,each%20layer%2C%20and%20so%20on)

- Weights and Biases: [Tune Hyperparameters](https://docs.wandb.ai/guides/sweeps)

- Weights and Biases: [Parameter Importance](https://docs.wandb.ai/ref/app/features/panels/parameter-importance)

_Image Source_: https://wandb.ai/