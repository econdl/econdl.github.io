---
layout: post
description: >
    Letcure 4: Covers transformer architecture in general and begins discussion on transformer language models 
title: The Transformer
category: intro
tags: [deep learning]
image: /assets/blogs/lecture_4.png
mathjax: true
published: true
---

## Topics

This post covers the fourth lecture in the course: "The Transformer." 

The transformer architecture revolutionized NLP and has since made substantial inroads in most
areas of deep learning (vision, audio, reinforcement learning…). This lecture will cover
substantial ground that will be foundational to the rest of the course. Please plan to devote
sufficient attention (no pun intended) to this material. 

## Lecture Video

<a href="https://www.youtube.com/watch?v=7L5XHLpzpMg&ab_channel=MelissaDell" target="_blank">
 <img src="http://img.youtube.com/vi/7L5XHLpzpMg/mqdefault.jpg" alt="Watch the video" width="560" height="315" />
</a>

[Lecture notes](https://www.dropbox.com/s/wkydtf1ihqgn1yi/lecture_transformers.pdf?dl=0)


## References Cited in Lecture 4: The Transformer and Transformer Language Models

#### Academic Papers

_The Original Transformer_

- Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Łukasz Kaiser, and Illia Polosukhin. ["Attention is all you need."](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) In Advances in Neural
Information Processing Systems, pp. 5998-6008. 2017.

_Transformer Language Models_

- Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. ["Bert: Pre-Training of
Deep Bidirectional Transformers for Language Understanding."](https://arxiv.org/pdf/1810.04805.pdf&usg=ALkJrhhzxlCL6yTht2BRmH9atgvKFxHsxQ) arXiv preprint
arXiv:1810.04805 (2018).

- Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. ["Roberta: A robustly optimized bert pretraining
approach."](https://arxiv.org/pdf/1907.11692.pdf%5C) arXiv preprint arXiv:1907.11692 (2019).

- Lan, Zhenzhong, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu
Soricut. ["Albert: A lite bert for self-supervised learning of language representations."](https://arxiv.org/pdf/1909.11942.pdf?fbclid=IwAR1gWlaWokv7Ys5JNkTgQ3Hw-) arXiv
preprint arXiv:1909.11942 (2019).

- Sanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf. ["DistilBERT, a distilled
version of BERT: smaller, faster, cheaper and lighter."](https://arxiv.org/pdf/1910.01108.pdf%3C/p%3E) arXiv preprint arXiv:1910.01108 (2019).

- Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan et al. ["Language models are few-shot learners."](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf) arXiv preprint
arXiv:2005.14165 (2020). (https://www.technologyreview.com/2020/09/23/1008729/openai-isgiving-microsoft-exclusive-access-to-its-gpt-3-language-model/); see also: Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. ["Language models
are unsupervised multitask learners."](https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf) OpenAI blog 1, no. 8 (2019): 9.

- Tay, Yi, Vinh Q. Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin,
Simon Baumgartner, Cong Yu, and Donald Metzler. ["Charformer: Fast character transformers
via gradient-based subword tokenization."](https://arxiv.org/pdf/2106.12672) arXiv preprint arXiv:2106.12672 (2021).
(Charformer)

- Nguyen, Dat Quoc, Thanh Vu, and Anh Tuan Nguyen. ["BERTweet: A pre-trained language
model for English Tweets."](https://arxiv.org/pdf/2005.10200) arXiv preprint arXiv:2005.10200 (2020). (BERTweet)

#### Other Resources

- [“The Annotated Transformer”](http://nlp.seas.harvard.edu/2018/04/03/attention.html) (a guide
annotating the paper with PyTorch implementation)
- [“The Illustrated Transformer”](http://jalammar.github.io/illustrated-transformer/)
- [“Transformers-based Encoder-Decoder Models”](https://huggingface.co/blog/encoder-decoder)
- [“Glass Box: The Transformer – Attention is All you Need”](https://glassboxmedicine.com/2019/08/15/the-transformer-attention-is-all-you-need/)
-[The Illustrated BERT, ELMo, and co.](http://jalammar.github.io/illustrated-bert/) (How NLP Cracked Transfer Learning):
-[The Illustrated GPT-2](http://jalammar.github.io/illustrated-gpt2/) (Visualizing Transformer Language Models)


#### Code Bases

- [OPT (Open Pre-trained Transfomers)](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT) from FAIR
- [Huggingface](https://github.com/huggingface) open source library with large variety of NLP models. See _Transformers_ repo for most applications. Most models referenced above (BERT, BERTweet, RoBERTa, DistilBERT) are implemented and easily accessed through Huggingface APIs. 

_Image Source_: Vaswani et. al. (2017) Attention Is All You Need.