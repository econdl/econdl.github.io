---
layout: post
description: >
    Letcure 6: Vision and Audio Transformers
title: Vision and Audio Transformers
category: intro
tags: [deep learning]
image: /assets/blogs/lecture_6.jpg
mathjax: true
published: true
---

## Topics

This post covers the sixth lecture in the course: "Vision and Audio Transformers." 

The transformer architecture has made major inroads in vision in recent years, with key advancements covered in this lecture. Similar recent advancements are also covered in the audio space.  

## Lecture Video

<a href="https://www.youtube.com/watch?v=kgfVvtD2KpM&ab_channel=MelissaDell" target="_blank">
 <img src="http://img.youtube.com/vi/kgfVvtD2KpM/mqdefault.jpg" alt="Watch the video" width="560" height="315" />
</a>

[Lecture notes](https://www.dropbox.com/s/0jsrbnco1d29q6t/lecture_vit.pdf?dl=0)


## References Cited in Lecture 4: The Transformer and Transformer Language Models

#### Academic Papers

_Original Vision Transfomer Paper_

- Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani et al. “[An image is worth 16x16 words: Transformers for image recognition at scale.](https://arxiv.org/pdf/2010.11929.pdf)” arXiv preprint arXiv:2010.11929 (2020). (Seminal ViT paper) 

_Further Work with Image Transformers_

- Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., & Jégou, H. (2021, July). [Training data-efficient image transformers & distillation through attention.](http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf) In International Conference on Machine Learning (pp. 10347-10357). PMLR. (DEIT)

- Grill, Jean-Bastien, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch et al. "[Bootstrap your own latent-a new approach to self-supervised learning.](https://proceedings.neurips.cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf)" Advances in neural information processing systems 33 (2020): 21271-21284

- Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., & Joulin, A. (2021). [Emerging properties in self-supervised vision transformers.](https://openaccess.thecvf.com/content/ICCV2021/papers/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.pdf) In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 9650-9660). (DINO)

- He, K., Chen, X., Xie, S., Li, Y., Dollár, P., & Girshick, R. (2022). [Masked autoencoders are scalable vision learners.](https://openaccess.thecvf.com/content/CVPR2022/papers/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.pdf) In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 16000-16009). (MAE)

- Ali, Alaaeldin, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev et al. "[Xcit: Cross-covariance image transformers.](https://proceedings.neurips.cc/paper/2021/file/a655fbe4b8d7439994aa37ddad80de56-Paper.pdf)" Advances in neural information processing systems 34 (2021): 20014-20027. (XCiT)

- Bai, Yutong, Jieru Mei, Alan L. Yuille, and Cihang Xie. "[Are Transformers more robust than CNNs?.](https://proceedings.neurips.cc/paper/2021/file/e19347e1c3ca0c0b97de5fb3b690855a-Paper.pdf)" Advances in Neural Information Processing Systems 34 (2021): 26831-26843.

- Wang, Zeyu, Yutong Bai, Yuyin Zhou, and Cihang Xie. "[Can CNNs Be More Robust Than Transformers?](https://arxiv.org/pdf/2206.03452.pdf)" arXiv preprint arXiv:2206.03452 (2022).

- Chen, Xinlei, Saining Xie, and Kaiming He. "[An empirical study of training self-supervised vision transformers.](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_An_Empirical_Study_of_Training_Self-Supervised_Vision_Transformers_ICCV_2021_paper.pdf)" In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9640-9649. 2021.

#### Other Resources

- [Vision Transformers Explained](https://blog.paperspace.com/vision-transformers/) R. Anand. (Ignore the final paragraphs headed "Transformers aren't mainstream yet")
- [Vision Transformers on paperswithcode](https://paperswithcode.com/methods/category/vision-transformer)
- [PyTorch Image Models (timm)](https://timm.fast.ai/). Open repository of image models, including XciT, Swin, ViT, etc
- [Benchmarking of timm models](https://colab.research.google.com/drive/1Mre2v11LX5Arkh-5VIKsSfm1rjayunZI?usp=sharing#scrollTo=OIy6v0UKATGH ) (Colab Notebook)
- [Which Image Models are Best?](https://www.kaggle.com/code/jhoward/which-image-models-are-best) (Kaggle) 

#### Code Bases

Using timm to implement these models is _strongly_ recommended. Here are a few official implementations: 

- [ViT](https://github.com/google-research/vision_transformer)
- [XCiT](https://github.com/facebookresearch/xcit)
- [SWIN Transformer](https://github.com/microsoft/Swin-Transformer)
- [MobileViT 1 and 2](https://github.com/apple/ml-cvnets)

_Image Source_: https://www.v7labs.com/blog/vision-transformer-guide