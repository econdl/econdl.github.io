---
layout: post
description: >
    Lecture 10: Multimodal Learning 
title: Multimodal Learning
category: intro
tags: [deep learning]
image: /assets/blogs/lecture_10.png
mathjax: true
published: true
---

## Topics

This post covers the tenth lecture in the course: "Multimodal Learning." 

Humans learn through multiple modalities, and combining modalities is also of relevance to a 
variety of economic applications. This lecture focuses primarily on vision language models.  

## Lecture Video

<a href="https://www.youtube.com/watch?v=Ym8B2TvFlNQ&ab_channel=MelissaDell" target="_blank">
 <img src="http://img.youtube.com/vi/Ym8B2TvFlNQ/mqdefault.jpg" alt="Watch the video" width="560" height="315" />
</a>

[Lecture notes](https://www.dropbox.com/s/6ysmz113ulrdszj/lecture_multimodal.pdf?dl=0)


## References Cited in Lecture 9: Multimodal Learning
 
Goh, Gabriel, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah. "[Multimodal neurons in artificial neural networks.](https://distill.pub/2021/multimodal-neurons/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)" Distill 6, no. 3 (2021): e30 
 
Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry et al. “[Learning transferable visual models from natural language supervision.](http://proceedings.mlr.press/v139/radford21a/radford21a.pdf)” International Conference on Machine Learning. PMLR, (2021).
 
Li, Liunian Harold, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. “[VisualBERT: A simple and performant baseline for vision and language.](https://arxiv.org/pdf/1908.03557.pdf&quot)” arXiv preprint arXiv:1908.03557 (2019). 
 
Wang, Zirui, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. "[Simvlm: Simple visual language model pretraining with weak supervision.](https://arxiv.org/pdf/2108.10904.pdf)" arXiv preprint arXiv:2108.10904 (2021). 
 
Tsimpoukelli, Maria, Jacob L. Menick, Serkan Cabi, S. M. Eslami, Oriol Vinyals, and Felix Hill. "[Multimodal few-shot learning with frozen language models.]()" Advances in Neural Information Processing Systems 34 (2021): 200-212. 
 
Mokady, Ron, Amir Hertz, and Amit H. Bermano. "[Clipcap: Clip prefix for image captioning.](https://proceedings.neurips.cc/paper/2021/file/01b7575c38dac42f3cfb7d500438b875-Paper.pdf)" arXiv preprint arXiv:2111.09734 (2021). 
 
Alayrac, Jean-Baptiste, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc et al. "[Flamingo: a visual language model for few-shot learning.](https://arxiv.org/pdf/2204.14198.pdf)" arXiv preprint arXiv:2204.14198 (2022). 
 
Nagrani, Arsha, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, and Chen Sun. "[Attention bottlenecks for multimodal fusion.](https://proceedings.neurips.cc/paper/2021/file/76ba9f564ebbc35b1014ac498fafadd0-Paper.pdf)" Advances in Neural Information Processing Systems 34 (2021): 14200-14213. See also blog post [here](https://ai.googleblog.com/2022/03/multimodal-bottleneck-transformer-mbt.html)  
 
Rust, Phillip, Jonas F. Lotz, Emanuele Bugliarello, Elizabeth Salesky, Miryam de Lhoneux, and Desmond Elliott. "[Language Modelling with Pixels.](https://arxiv.org/pdf/2207.06991.pdf)" arXiv preprint arXiv:2207.06991 (2022). 

#### Other Resources
Generalized Vision Language Models (a highly informative blog post overview) https://lilianweng.github.io/posts/2022-06-09-vlm/  

[OpenAI blog about Clip](https://openai.com/blog/clip/)

[Twitter thread by Christopher Manning](https://twitter.com/chrmanning/status/1572640802450604032?t=kWeOuXj7lk5kY-BTncSbyw&s=09)

_Image Source_: https://towardsdatascience.com/multimodal-deep-learning-ce7d1d994f4