---
layout: post
description: >
    Lecture 3: Covers language models before transformers: RNNs, Seq2Seq, LSTM models
title: Language Modeling, Recurrent Neural Nets and Seq2Seq   
category: intro
tags: [deep learning]
image: /assets/blogs/lecture_3.png
mathjax: true
published: true
---

## Topics

This post covers the third lecture in the course: "Language Modeling, Recurrent Neural Nets and Seq2Seq." 

TThis lecture will cover the history of language modeling prior to transformers. Even though you
should typically use a transformer-based language model, this lecture will provide valuable
context for understanding NLP and how the field has evolved. It will also introduce sequence-tosequence models and attention, key pre-requisites for understanding transformers and various
applications.

The Lecture is broken up into three parts: 
 - _Language Modeling_, covering early examples of language modeling
 - _Word Embeddings_, covering GloVe, Word2Vec, etc
 - _Seq2Seq_, covering Sequence to Sequence models (LSTMs)

## Lecture Videos

_Language Modeling_

<a href="https://www.youtube.com/watch?v=4P_qsKMsE2E&ab_channel=MelissaDell" target="_blank">
 <img src="http://img.youtube.com/vi/4P_qsKMsE2E/mqdefault.jpg" alt="Watch the video" width="560" height="315" />
</a>


<iframe width="560" height="315" src="https://www.youtube.com/embed/U0zkQVQrxdw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

_Word Embeddings_

<a href="https://www.youtube.com/watch?v=wjoDqcEXIJ8&ab_channel=MelissaDell" target="_blank">
 <img src="http://img.youtube.com/vi/wjoDqcEXIJ8/mqdefault.jpg" alt="Watch the video" width="560" height="315" />
</a>

_Sequence to Sequence Models_

<a href="https://www.youtube.com/watch?v=vYi5btbpSgI&ab_channel=MelissaDell" target="_blank">
 <img src="http://img.youtube.com/vi/vYi5btbpSgI/mqdefault.jpg" alt="Watch the video" width="560" height="315" />
</a>

Lecture notes: [Language Modeling](https://www.dropbox.com/s/1witjrhf87jeo4e/lecture_rnns.pdf?dl=0), [Word Embeddings](https://www.dropbox.com/s/7ah57lns4eil9fv/lecture_words.pdf?dl=0), [Seq2Seq](https://www.dropbox.com/s/83wl89il8nwc2zp/lecture_seq2seq.pdf?dl=0)


## References Cited in Lecture 3: Language Modeling, Recurrent Neural Nets and Seq2Seq

#### Academic Papers

- Jurafsky, Daniel and James Martin, [“N-Gram Language Models”](https://web.stanford.edu/~jurafsky/slp3/3.pdf) Speech and Language
Processing, 2020 .

- Bengio, Yoshua, Patrice Simard, and Paolo Frasconi. ["Learning long-term dependencies with
gradient descent is difficult."](http://www.comp.hkbu.edu.hk/~markus/teaching/comp7650/tnn-94-gradient.pdf) IEEE transactions on neural networks 5, no. 2 (1994): 157-166.

- Hochreiter, Sepp, and Jürgen Schmidhuber. ["Long short-term memory."](https://blog.xpgreat.com/file/lstm.pdf) Neural computation 9,
no. 8 (1997): 1735-1780.

- Greff, Klaus, Rupesh K. Srivastava, Jan Koutník, Bas R. Steunebrink, and Jürgen Schmidhuber.
["LSTM: A search space odyssey."](https://arxiv.org/pdf/1503.04069.pdf?fbclid=IwAR377Jhphz_xGSSThcqGUlAx8OJc_gU6Zwq8dABHOdS4WNOPRXA5LcHOjUg) IEEE transactions on neural networks and learning
systems 28, no. 10 (2016): 2222-2232.

- Chung, Junyoung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. ["Empirical
evaluation of gated recurrent neural networks on sequence modeling."](https://arxiv.org/pdf/1412.3555) arXiv preprint
arXiv:1412.3555 (2014).

- Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. ["Sequence to sequence learning with neural
networks."](https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf) arXiv preprint arXiv:1409.3215 (2014).

- Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. ["Neural machine translation by
jointly learning to align and translate."](https://arxiv.org/pdf/1409.0473.pdf?utm_source=ColumnsChannel) arXiv preprint arXiv:1409.0473 (2014).

- Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. ["Distributed
representations of words and phrases and their compositionality."](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf) Advances in Neural
Information Processing Systems 26 (2013): 3111-3119. (Notes: this paper developed the
word2vec model commonly used in NLP)

- Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. ["Glove: Global vectors for
word representation."](https://aclanthology.org/D14-1162.pdf) In Proceedings of the 2014 conference on empirical methods in natural
language processing (EMNLP), pp. 1532-1543. 2014.

#### Other Resources

- Olah, Chris, and Shan Carter. ["Attention and augmented recurrent neural networks."](https://distill.pub/2016/augmented-rnns/) Distill 1,
no. 9 (2016): e1. 

- Olah, Christopher. [“Deep Learning, NLP, and Representation”](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/), 2014.

#### Code Bases

- Pytorch Examples: [Classifying MNIST digits with an RNN](https://github.com/pytorch/examples/tree/main/mnist_rnn)

_Image Source_: https://thorirmar.com/post/insight_into_lstm/
