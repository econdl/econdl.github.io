---
layout: post
description: >
    Lecture 5: Transformer language models 
title: More on Transformer Language Models
category: intro
tags: [deep learning]
image: /assets/blogs/lecture_5.png
mathjax: true
published: true
---

## Topics

This post covers the fifth lecture in the course: "More on Transformer Language Models." 

This lecture will continue our discussion of transformer language models, focusing on interpretation and visualization of textual embeddings and the challenges – and interesting questions – raised by evolving language. 

## Lecture Video

_Part 1_

<a href="https://www.youtube.com/watch?v=Pe67rtgqCqI&ab_channel=MelissaDell" target="_blank">
 <img src="http://img.youtube.com/vi/Pe67rtgqCqI/mqdefault.jpg" alt="Watch the video" width="560" height="315" />
</a>

_Part 2_

<a href="https://www.youtube.com/watch?v=GgajRRH8Efg&ab_channel=MelissaDell" target="_blank">
 <img src="http://img.youtube.com/vi/GgajRRH8Efg/mqdefault.jpg" alt="Watch the video" width="560" height="315" />
</a>

[Lecture notes 1](https://www.dropbox.com/s/huwe3209ybxiuzr/lecture_transformerLMs.pdf?dl=0)
[Lecture notes 2](https://www.dropbox.com/s/5k79dr2ctpy44a7/lecture_moreLMs.pdf?dl=0)

## References Cited in Lecture 5: More on Transformer Language Models

#### Academic Papers

_Interpreting Textual Embeddings_

- McInnes, Leland, John Healy, and James Melville. "[Umap: Uniform manifold approximation and projection for dimension reduction.](https://arxiv.org/pdf/1802.03426.pdf)" arXiv preprint arXiv:1802.03426 (2018). 

- Köhn, Arne. "[What’s in an embedding? Analyzing word embeddings through multilingual evaluation.](https://edoc.sub.uni-hamburg.de/informatik/volltexte/2015/213/pdf/Koehn_whatsinanembedding.pdf)" In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 2067-2073. 2015.

- Rogers, Anna, Olga Kovaleva, and Anna Rumshisky. "[A primer in bertology: What we know about how bert works.](https://watermark.silverchair.com/tacl_a_00349.pdf?)" Transactions of the Association for Computational Linguistics 8 (2020): 842-866.

- Wiedemann, Gregor, Steffen Remus, Avi Chawla, and Chris Biemann. "[Does BERT make any sense? Interpretable word sense disambiguation with contextualized embeddings.](https://arxiv.org/pdf/1909.10430.pdf)" arXiv preprint arXiv:1909.10430 (2019).

- Coenen, Andy, Emily Reif, Ann Yuan, Been Kim, Adam Pearce, Fernanda Viégas, and Martin Wattenberg. "[Visualizing and measuring the geometry of bert.](https://proceedings.neurips.cc/paper/2019/file/159c1ffe5b61b41b3c4d8f4c2150f6c4-Paper.pdf)" arXiv preprint arXiv:1906.02715 (2019).

- Ethayarajh, Kawin. "[How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings.](https://arxiv.org/pdf/1909.00512.pdf)" arXiv preprint arXiv:1909.00512 (2019).

- Merchant, Amil, Elahe Rahimtoroghi, Ellie Pavlick, and Ian Tenney. "[What Happens to BERT Embeddings During Fine-tuning?](https://arxiv.org/pdf/2004.14448.pdf)" arXiv preprint arXiv:2004.14448 (2020).

_Changing Language_

- Manjavacas, Enrique, and Lauren Fonteyn. "[Macberth: Development and evaluation of a historically pre-trained language model for english (1450-1950).](https://aclanthology.org/2021.nlp4dh-1.4.pdf)" In Proceedings of the Workshop on Natural Language Processing for Digital Humanities, pp. 23-36. 2021.

- Amba Hombaiah, Spurthi, Tao Chen, Mingyang Zhang, Michael Bendersky, and Marc Najork. "[Dynamic language models for continuously evolving content.](https://dl.acm.org/doi/pdf/10.1145/3447548.3467162)" In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 2514-2524. 2021.

- Manjavacas, Enrique, and Lauren Fonteyn. "[Adapting vs Pre-training Language Models for Historical Languages.](https://jdmdh.episciences.org/9690/pdf)" (2022).

- Soni, Sandeep, David Bamman, and Jacob Eisenstein. "[Predicting Long-Term Citations from Short-Term Linguistic Influence.](https://arxiv.org/pdf/2210.13628.pdf)" Findings of the Association for Computational Linguistics: EMNLP 2022 (2022).



#### Other Resources

- Tensorflow Embedding Projector: http://projector.tensorflow.org

#### Code Bases

_Historical Language Models_

- [Huggingface](https://github.com/huggingface) open source library with large variety of NLP models. Includes MacBERTh and several other historically trained or finetuned language models

_Image Source_: Devlin, J., Chang, M., Lee, K., Toutanova, K. (2018) BERT: Pre-training of Deep Bidirectional Transformers for Lanaugage Understanding