<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>EconDL | Linking Representations with Multimodal Contrastive Learning</title>
    <meta name="description" content="Arora, Abhishek, Xinmei Yang, Shao-Yu Jheng, and Melissa Dell. “Linking Representations with Multimodal Contrastive Learning," [Paper](https://scholar.harvard.edu/sites/scholar.harvard.edu/files/dell/files/rl.pdf)
">

    <meta property="og:title" content="Linking Representations with Multimodal Contrastive Learning">
    <meta property="og:type" content="website">
    <meta property="og:url" content="/wp/390clippings">
    <meta property="og:description" content="Arora, Abhishek, Xinmei Yang, Shao-Yu Jheng, and Melissa Dell. “Linking Representations with Multimodal Contrastive Learning," [Paper](https://scholar.harvard.edu/sites/scholar.harvard.edu/files/dell/files/rl.pdf)
">
    <meta property="og:site_name" content="">

    <meta name="twitter:card" content="summary">
    <meta name="twitter:url" content="/wp/390clippings">
    <meta name="twitter:title" content="Linking Representations with Multimodal Contrastive Learning">
    <meta name="twitter:description" content="Arora, Abhishek, Xinmei Yang, Shao-Yu Jheng, and Melissa Dell. “Linking Representations with Multimodal Contrastive Learning," [Paper](https://scholar.harvard.edu/sites/scholar.harvard.edu/files/dell/files/rl.pdf)
">

    <!-- TODO: Adding page thumbnail for social meida -->

    <!-- Canonical URL -->
    <link rel="canonical" href="/wp/390clippings">

    <!-- Stylesheet -->
    <link rel="stylesheet" href="/assets/css/bulma.css">
    <link rel="stylesheet" href="/assets/css/syntax/manni.css">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

    <script defer src="https://use.fontawesome.com/releases/v5.3.1/js/all.js"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
          inlineMath: [ ['$','$'] ],
          displayMath: [ ['$$','$$'] ],
          processEscapes: true,
        }
      });
    </script>
    
    <script type="text/javascript"
            src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    
    <!-- Add link to h2 ... h6 -->
    <!-- Ref: https://ben.balter.com/2014/03/13/pages-anchor-links/ -->
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script>
      $(function() {
        return $("h2, h3, h4, h5, h6").each(function(i, el) {
          var $el, icon, id;
          $el = $(el);
          id = $el.attr('id');
          icon = '<i class="fas fa-link"></i>';
          if (id) {
            return $el.append($("<a />").addClass("header-link").attr("href", "#" + id).html(icon));
          }
        });
      });
    </script>
    
    <script src="/assets/js/bulma-collapsible.min.js"></script>

    <!-- Google Analytics -->
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', '');
</script>


</head>

<body>

    <div class="section">            
        <div class="container">
            
                <nav class="navbar" role="navigation">
    <div class="navbar-brand">
        <a class="navbar-item site-title" href="/">
            <strong>EconDL</strong>
        </a>

        <div class="navbar-burger" data-target="navbar-main"
            onclick="document.querySelector('.navbar-menu').classList.toggle('is-active');">
            <span></span>
            <span></span>
            <span></span>
        </div>
    </div>

    <div class="navbar-menu" id="navbar-main">
        <div class="navbar-end">
            <!-- navbar items -->
            
                
                    <a href="/" class="navbar-item"></a>
                
            
                
                    <a href="/packages.html" class="navbar-item">Packages</a>
                
            
                
                    <a href="/datasets.html" class="navbar-item">Datasets</a>
                
            
            <div class="navbar-item">
                
            </div>
        </div>
    </div>
</nav>
<div class="is-divider" style="margin-top: 0.5rem; margin-bottom: 3rem;"></div>
  
            
            <article class="content" style="margin-top: 3rem; margin-bottom: 4rem">
  <header class="article-header">
      <div class="columns">
        <div class="column is-12">
          <h1 style="letter-spacing: -0.02rem">Linking Representations with Multimodal Contrastive Learning</h1>
           
<div class="article-list-footer">
    
    <!-- <span class="article-list-divider">-</span>
    <span class="field is-grouped">
        
    </span> -->
</div>
        </div>
      </div>
  </header>
  <div class="is-divider" style="margin-top: 0.5rem; margin-bottom: 1.5rem;"></div>
  <p><strong>Abstract:</strong> Many applications require grouping instances contained in diverse document datasets into classes. Most widely used methods do not employ deep learning and do not exploit the inherently multimodal nature of documents. Notably, record linkage is typically conceptualized as a string-matching problem. This study develops CLIPPINGS, (Contrastively Linking Pooled Pre-trained Embeddings), a multimodal framework for record linkage. CLIPPINGS employs end-to-end training of symmetric vision and language bi-encoders, aligned through contrastive language-image pre-training, to learn a metric space where the pooled image-text representation for a given instance is close to representations in the same class and distant from representations in different classes. At inference time, instances can be linked by retrieving their nearest neighbor from an offline exemplar embedding index or by clustering their representations. The study examines two challenging applications: constructing comprehensive supply chains for mid-20th century Japan through linking firm level financial records - with each firm name represented by its crop in the document image and the corresponding OCR - and detecting which image-caption pairs in a massive corpus of historical U.S. newspapers came from the same underlying photo wire source. CLIPPINGS outperforms widely used string matching methods by a wide margin and also outperforms unimodal methods. Moreover, a purely self-supervised model trained on only image-OCR pairs also outperforms popular string-matching methods without requiring any labels.</p>

<p><a href="https://scholar.harvard.edu/sites/scholar.harvard.edu/files/dell/files/rl.pdf">Paper</a></p>

</article>

        </div>
    </div>
    

</body>

</html>