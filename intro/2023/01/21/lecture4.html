<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>EconDL | The Transformer</title>
    <meta name="description" content="Lecture 4: Covers transformer architecture in general and begins discussion on transformer language models 
">

    <meta property="og:title" content="The Transformer">
    <meta property="og:type" content="website">
    <meta property="og:url" content="/intro/2023/01/21/lecture4.html">
    <meta property="og:description" content="Lecture 4: Covers transformer architecture in general and begins discussion on transformer language models 
">
    <meta property="og:site_name" content="">

    <meta name="twitter:card" content="summary">
    <meta name="twitter:url" content="/intro/2023/01/21/lecture4.html">
    <meta name="twitter:title" content="The Transformer">
    <meta name="twitter:description" content="Lecture 4: Covers transformer architecture in general and begins discussion on transformer language models 
">

    <!-- TODO: Adding page thumbnail for social meida -->

    <!-- Canonical URL -->
    <link rel="canonical" href="/intro/2023/01/21/lecture4.html">

    <!-- Stylesheet -->
    <link rel="stylesheet" href="/assets/css/bulma.css">
    <link rel="stylesheet" href="/assets/css/syntax/manni.css">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

    <script defer src="https://use.fontawesome.com/releases/v5.3.1/js/all.js"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
          inlineMath: [ ['$','$'] ],
          displayMath: [ ['$$','$$'] ],
          processEscapes: true,
        }
      });
    </script>
    
    <script type="text/javascript"
            src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    
    <!-- Add link to h2 ... h6 -->
    <!-- Ref: https://ben.balter.com/2014/03/13/pages-anchor-links/ -->
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script>
      $(function() {
        return $("h2, h3, h4, h5, h6").each(function(i, el) {
          var $el, icon, id;
          $el = $(el);
          id = $el.attr('id');
          icon = '<i class="fas fa-link"></i>';
          if (id) {
            return $el.append($("<a />").addClass("header-link").attr("href", "#" + id).html(icon));
          }
        });
      });
    </script>
    
    <script src="/assets/js/bulma-collapsible.min.js"></script>

    <!-- Google Analytics -->
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', '');
</script>


</head>

<body>

    <div class="section">            
        <div class="container">
            
                <nav class="navbar" role="navigation">
    <div class="navbar-brand">
        <a class="navbar-item site-title" href="/">
            <strong>EconDL</strong>
        </a>

        <div class="navbar-burger" data-target="navbar-main"
            onclick="document.querySelector('.navbar-menu').classList.toggle('is-active');">
            <span></span>
            <span></span>
            <span></span>
        </div>
    </div>

    <div class="navbar-menu" id="navbar-main">
        <div class="navbar-end">
            <!-- navbar items -->
            
                
                    <a href="/" class="navbar-item"></a>
                
            
                
                    <a href="/packages.html" class="navbar-item">Packages</a>
                
            
                
                    <a href="/datasets.html" class="navbar-item">Datasets</a>
                
            
            <div class="navbar-item">
                
            </div>
        </div>
    </div>
</nav>
<div class="is-divider" style="margin-top: 0.5rem; margin-bottom: 3rem;"></div>
  
            
            <article class="content" style="margin-top: 3rem">
  <div class="columns">
    <div class="column is-2 is-hidden-mobile">
    </div>
    <div class="column is-8">
        <header class="article-header">
            <h1>The Transformer</h1>
            <div class="article-list-footer">
    <span class="article-list-date">
        January 21, 2023
    </span>
    <span class="article-list-divider">-</span>
    <span class="article-list-minutes">
        
        
        2 minute read
        
    </span>
    </span>
    
    <span class="article-list-divider">-</span>
    <span class="article-list-cat">Category:</span>
    
    <span class="field">
        
        <a href="/category/intro" class="article-list-item">
            Intro
        </a>
        
        
    </span>
    
    <span class="article-list-divider">-</span>
    <span class="article-list-cat">Tags:</span>
    
    <span class="field">
        
        <a href="/tag/deep learning" class="article-list-item">
            Deep learning
        </a>
        
        
    </span>
</div>
        </header>
        <h2 id="topics">Topics</h2>

<p>This post covers the fourth lecture in the course: “The Transformer.”</p>

<p>The transformer architecture revolutionized NLP and has since made substantial inroads in most
areas of deep learning (vision, audio, reinforcement learning…). This lecture will cover
substantial ground that will be foundational to the rest of the course. Please plan to devote
sufficient attention (no pun intended) to this material.</p>

<h2 id="lecture-video">Lecture Video</h2>

<p><a href="https://www.youtube.com/watch?v=7L5XHLpzpMg&amp;ab_channel=MelissaDell" target="_blank">
 <img src="http://img.youtube.com/vi/7L5XHLpzpMg/mqdefault.jpg" alt="Watch the video" width="560" height="315" />
</a></p>

<p><a href="https://www.dropbox.com/s/wkydtf1ihqgn1yi/lecture_transformers.pdf?dl=0">Lecture notes</a></p>

<h2 id="references-cited-in-lecture-4-the-transformer-and-transformer-language-models">References Cited in Lecture 4: The Transformer and Transformer Language Models</h2>

<h4 id="academic-papers">Academic Papers</h4>

<p><em>The Original Transformer</em></p>

<ul>
  <li>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Łukasz Kaiser, and Illia Polosukhin. <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">“Attention is all you need.”</a> In Advances in Neural
Information Processing Systems, pp. 5998-6008. 2017.</li>
</ul>

<p><em>Transformer Language Models</em></p>

<ul>
  <li>
    <p>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. <a href="https://arxiv.org/pdf/1810.04805.pdf&amp;usg=ALkJrhhzxlCL6yTht2BRmH9atgvKFxHsxQ">“Bert: Pre-Training of
Deep Bidirectional Transformers for Language Understanding.”</a> arXiv preprint
arXiv:1810.04805 (2018).</p>
  </li>
  <li>
    <p>Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. <a href="https://arxiv.org/pdf/1907.11692.pdf%5C">“Roberta: A robustly optimized bert pretraining
approach.”</a> arXiv preprint arXiv:1907.11692 (2019).</p>
  </li>
  <li>
    <p>Lan, Zhenzhong, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu
Soricut. <a href="https://arxiv.org/pdf/1909.11942.pdf?fbclid=IwAR1gWlaWokv7Ys5JNkTgQ3Hw-">“Albert: A lite bert for self-supervised learning of language representations.”</a> arXiv
preprint arXiv:1909.11942 (2019).</p>
  </li>
  <li>
    <p>Sanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf. <a href="https://arxiv.org/pdf/1910.01108.pdf%3C/p%3E">“DistilBERT, a distilled
version of BERT: smaller, faster, cheaper and lighter.”</a> arXiv preprint arXiv:1910.01108 (2019).</p>
  </li>
  <li>
    <p>Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan et al. <a href="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">“Language models are few-shot learners.”</a> arXiv preprint
arXiv:2005.14165 (2020). (https://www.technologyreview.com/2020/09/23/1008729/openai-isgiving-microsoft-exclusive-access-to-its-gpt-3-language-model/); see also: Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. <a href="https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf">“Language models
are unsupervised multitask learners.”</a> OpenAI blog 1, no. 8 (2019): 9.</p>
  </li>
  <li>
    <p>Tay, Yi, Vinh Q. Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin,
Simon Baumgartner, Cong Yu, and Donald Metzler. <a href="https://arxiv.org/pdf/2106.12672">“Charformer: Fast character transformers
via gradient-based subword tokenization.”</a> arXiv preprint arXiv:2106.12672 (2021).
(Charformer)</p>
  </li>
  <li>
    <p>Nguyen, Dat Quoc, Thanh Vu, and Anh Tuan Nguyen. <a href="https://arxiv.org/pdf/2005.10200">“BERTweet: A pre-trained language
model for English Tweets.”</a> arXiv preprint arXiv:2005.10200 (2020). (BERTweet)</p>
  </li>
</ul>

<h4 id="other-resources">Other Resources</h4>

<ul>
  <li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">“The Annotated Transformer”</a> (a guide
annotating the paper with PyTorch implementation)</li>
  <li><a href="http://jalammar.github.io/illustrated-transformer/">“The Illustrated Transformer”</a></li>
  <li><a href="https://huggingface.co/blog/encoder-decoder">“Transformers-based Encoder-Decoder Models”</a></li>
  <li><a href="https://glassboxmedicine.com/2019/08/15/the-transformer-attention-is-all-you-need/">“Glass Box: The Transformer – Attention is All you Need”</a>
-<a href="http://jalammar.github.io/illustrated-bert/">The Illustrated BERT, ELMo, and co.</a> (How NLP Cracked Transfer Learning):
-<a href="http://jalammar.github.io/illustrated-gpt2/">The Illustrated GPT-2</a> (Visualizing Transformer Language Models)</li>
</ul>

<h4 id="code-bases">Code Bases</h4>

<ul>
  <li><a href="https://github.com/facebookresearch/metaseq/tree/main/projects/OPT">OPT (Open Pre-trained Transfomers)</a> from FAIR</li>
  <li><a href="https://github.com/huggingface">Huggingface</a> open source library with large variety of NLP models. See <em>Transformers</em> repo for most applications. Most models referenced above (BERT, BERTweet, RoBERTa, DistilBERT) are implemented and easily accessed through Huggingface APIs.</li>
</ul>

<p><em>Image Source</em>: Vaswani et. al. (2017) Attention Is All You Need.</p>

    </div>
  </div>
</article>

        </div>
    </div>
    

</body>

</html>