<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>EconDL | Vision and Audio Transformers</title>
    <meta name="description" content="Lecture 6: Vision and Audio Transformers
">

    <meta property="og:title" content="Vision and Audio Transformers">
    <meta property="og:type" content="website">
    <meta property="og:url" content="/intro/2023/01/23/lecture6.html">
    <meta property="og:description" content="Lecture 6: Vision and Audio Transformers
">
    <meta property="og:site_name" content="">

    <meta name="twitter:card" content="summary">
    <meta name="twitter:url" content="/intro/2023/01/23/lecture6.html">
    <meta name="twitter:title" content="Vision and Audio Transformers">
    <meta name="twitter:description" content="Lecture 6: Vision and Audio Transformers
">

    <!-- TODO: Adding page thumbnail for social meida -->

    <!-- Canonical URL -->
    <link rel="canonical" href="/intro/2023/01/23/lecture6.html">

    <!-- Stylesheet -->
    <link rel="stylesheet" href="/assets/css/bulma.css">
    <link rel="stylesheet" href="/assets/css/syntax/manni.css">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

    <script defer src="https://use.fontawesome.com/releases/v5.3.1/js/all.js"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
          inlineMath: [ ['$','$'] ],
          displayMath: [ ['$$','$$'] ],
          processEscapes: true,
        }
      });
    </script>
    
    <script type="text/javascript"
            src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    
    <!-- Add link to h2 ... h6 -->
    <!-- Ref: https://ben.balter.com/2014/03/13/pages-anchor-links/ -->
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script>
      $(function() {
        return $("h2, h3, h4, h5, h6").each(function(i, el) {
          var $el, icon, id;
          $el = $(el);
          id = $el.attr('id');
          icon = '<i class="fas fa-link"></i>';
          if (id) {
            return $el.append($("<a />").addClass("header-link").attr("href", "#" + id).html(icon));
          }
        });
      });
    </script>
    
    <script src="/assets/js/bulma-collapsible.min.js"></script>

    <!-- Google Analytics -->
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', '');
</script>


</head>

<body>

    <div class="section">            
        <div class="container">
            
                <nav class="navbar" role="navigation">
    <div class="navbar-brand">
        <a class="navbar-item site-title" href="/">
            <strong>EconDL</strong>
        </a>

        <div class="navbar-burger" data-target="navbar-main"
            onclick="document.querySelector('.navbar-menu').classList.toggle('is-active');">
            <span></span>
            <span></span>
            <span></span>
        </div>
    </div>

    <div class="navbar-menu" id="navbar-main">
        <div class="navbar-end">
            <!-- navbar items -->
            
                
                    <a href="/" class="navbar-item"></a>
                
            
                
                    <a href="/packages.html" class="navbar-item">Packages</a>
                
            
                
                    <a href="/datasets.html" class="navbar-item">Datasets</a>
                
            
            <div class="navbar-item">
                
            </div>
        </div>
    </div>
</nav>
<div class="is-divider" style="margin-top: 0.5rem; margin-bottom: 3rem;"></div>
  
            
            <article class="content" style="margin-top: 3rem">
  <div class="columns">
    <div class="column is-2 is-hidden-mobile">
    </div>
    <div class="column is-8">
        <header class="article-header">
            <h1>Vision and Audio Transformers</h1>
            <div class="article-list-footer">
    <span class="article-list-date">
        January 23, 2023
    </span>
    <span class="article-list-divider">-</span>
    <span class="article-list-minutes">
        
        
        2 minute read
        
    </span>
    </span>
    
    <span class="article-list-divider">-</span>
    <span class="article-list-cat">Category:</span>
    
    <span class="field">
        
        <a href="/category/intro" class="article-list-item">
            Intro
        </a>
        
        
    </span>
    
    <span class="article-list-divider">-</span>
    <span class="article-list-cat">Tags:</span>
    
    <span class="field">
        
        <a href="/tag/deep learning" class="article-list-item">
            Deep learning
        </a>
        
        
    </span>
</div>
        </header>
        <h2 id="topics">Topics</h2>

<p>This post covers the sixth lecture in the course: “Vision and Audio Transformers.”</p>

<p>The transformer architecture has made major inroads in vision in recent years, with key advancements covered in this lecture. Similar recent advancements are also covered in the audio space.</p>

<h2 id="lecture-video">Lecture Video</h2>

<p><a href="https://www.youtube.com/watch?v=kgfVvtD2KpM&amp;ab_channel=MelissaDell" target="_blank">
 <img src="http://img.youtube.com/vi/kgfVvtD2KpM/mqdefault.jpg" alt="Watch the video" width="560" height="315" />
</a></p>

<p><a href="https://www.dropbox.com/s/0jsrbnco1d29q6t/lecture_vit.pdf?dl=0">Lecture notes</a></p>

<h2 id="references-cited-in-lecture-4-the-transformer-and-transformer-language-models">References Cited in Lecture 4: The Transformer and Transformer Language Models</h2>

<h4 id="academic-papers">Academic Papers</h4>

<p><em>Original Vision Transfomer Paper</em></p>

<ul>
  <li>Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani et al. “<a href="https://arxiv.org/pdf/2010.11929.pdf">An image is worth 16x16 words: Transformers for image recognition at scale.</a>” arXiv preprint arXiv:2010.11929 (2020). (Seminal ViT paper)</li>
</ul>

<p><em>Further Work with Image Transformers</em></p>

<ul>
  <li>
    <p>Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., &amp; Jégou, H. (2021, July). <a href="http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf">Training data-efficient image transformers &amp; distillation through attention.</a> In International Conference on Machine Learning (pp. 10347-10357). PMLR. (DEIT)</p>
  </li>
  <li>
    <p>Grill, Jean-Bastien, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch et al. “<a href="https://proceedings.neurips.cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf">Bootstrap your own latent-a new approach to self-supervised learning.</a>” Advances in neural information processing systems 33 (2020): 21271-21284</p>
  </li>
  <li>
    <p>Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., &amp; Joulin, A. (2021). <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.pdf">Emerging properties in self-supervised vision transformers.</a> In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 9650-9660). (DINO)</p>
  </li>
  <li>
    <p>He, K., Chen, X., Xie, S., Li, Y., Dollár, P., &amp; Girshick, R. (2022). <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.pdf">Masked autoencoders are scalable vision learners.</a> In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 16000-16009). (MAE)</p>
  </li>
  <li>
    <p>Ali, Alaaeldin, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev et al. “<a href="https://proceedings.neurips.cc/paper/2021/file/a655fbe4b8d7439994aa37ddad80de56-Paper.pdf">Xcit: Cross-covariance image transformers.</a>” Advances in neural information processing systems 34 (2021): 20014-20027. (XCiT)</p>
  </li>
  <li>
    <p>Bai, Yutong, Jieru Mei, Alan L. Yuille, and Cihang Xie. “<a href="https://proceedings.neurips.cc/paper/2021/file/e19347e1c3ca0c0b97de5fb3b690855a-Paper.pdf">Are Transformers more robust than CNNs?.</a>” Advances in Neural Information Processing Systems 34 (2021): 26831-26843.</p>
  </li>
  <li>
    <p>Wang, Zeyu, Yutong Bai, Yuyin Zhou, and Cihang Xie. “<a href="https://arxiv.org/pdf/2206.03452.pdf">Can CNNs Be More Robust Than Transformers?</a>” arXiv preprint arXiv:2206.03452 (2022).</p>
  </li>
  <li>
    <p>Chen, Xinlei, Saining Xie, and Kaiming He. “<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_An_Empirical_Study_of_Training_Self-Supervised_Vision_Transformers_ICCV_2021_paper.pdf">An empirical study of training self-supervised vision transformers.</a>” In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9640-9649. 2021.</p>
  </li>
</ul>

<h4 id="other-resources">Other Resources</h4>

<ul>
  <li><a href="https://blog.paperspace.com/vision-transformers/">Vision Transformers Explained</a> R. Anand. (Ignore the final paragraphs headed “Transformers aren’t mainstream yet”)</li>
  <li><a href="https://paperswithcode.com/methods/category/vision-transformer">Vision Transformers on paperswithcode</a></li>
  <li><a href="https://timm.fast.ai/">PyTorch Image Models (timm)</a>. Open repository of image models, including XciT, Swin, ViT, etc</li>
  <li><a href="https://colab.research.google.com/drive/1Mre2v11LX5Arkh-5VIKsSfm1rjayunZI?usp=sharing#scrollTo=OIy6v0UKATGH">Benchmarking of timm models</a> (Colab Notebook)</li>
  <li><a href="https://www.kaggle.com/code/jhoward/which-image-models-are-best">Which Image Models are Best?</a> (Kaggle)</li>
</ul>

<h4 id="code-bases">Code Bases</h4>

<p>Using timm to implement these models is <em>strongly</em> recommended. Here are a few official implementations:</p>

<ul>
  <li><a href="https://github.com/google-research/vision_transformer">ViT</a></li>
  <li><a href="https://github.com/facebookresearch/xcit">XCiT</a></li>
  <li><a href="https://github.com/microsoft/Swin-Transformer">SWIN Transformer</a></li>
  <li><a href="https://github.com/apple/ml-cvnets">MobileViT 1 and 2</a></li>
</ul>

<p><em>Image Source</em>: https://www.v7labs.com/blog/vision-transformer-guide</p>

    </div>
  </div>
</article>

        </div>
    </div>
    

</body>

</html>